{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d42d9ce",
   "metadata": {},
   "source": [
    "# <center>Projet d'optimisation pour la classification </center>\n",
    "\n",
    "Ce notebook détaille les étapes de la construction d'un réseau de neurones pour la classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3dcb4a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# import matplotlib  # maybe useful Ubuntu rodo ISIMA\n",
    "# matplotlib.use('Qt5Agg') # maybe useful Ubuntu rodo ISIMA\n",
    "import csv\n",
    "\n",
    "from typing import Callable, List\n",
    "\n",
    "from optimcourse.gradient_descent import gradient_descent, gradient_finite_diff\n",
    "from optimcourse.optim_utilities import print_rec\n",
    "from optimcourse.forward_propagation import (\n",
    "    forward_propagation,\n",
    "    create_weights,\n",
    "    vector_to_weights,\n",
    "    weights_to_vector)\n",
    "from optimcourse.activation_functions import (\n",
    "    relu,\n",
    "    sigmoid,\n",
    "    linear,\n",
    "    leaky_relu\n",
    ")\n",
    "from optimcourse.test_functions import (\n",
    "    linear_function,\n",
    "    ackley,\n",
    "    sphere,\n",
    "    quadratic,\n",
    "    rosen,\n",
    "    L1norm,\n",
    "    sphereL1\n",
    ")\n",
    "from optimcourse.restarted_gradient_descent import restarted_gradient_descent\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61bcab2d",
   "metadata": {},
   "source": [
    "## Importation des données\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1942e77e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data file\n",
    "with open('./donnees_mensuration.csv', mode='r', encoding='utf-8') as file:\n",
    "    csv_reader = csv.reader(file,delimiter=';')\n",
    "    header = next(csv_reader)\n",
    "    meas_data = []\n",
    "    for row in csv_reader:\n",
    "        # Convert numeric strings to float where applicable\n",
    "        processed_row = []\n",
    "        for item in row:\n",
    "            try:\n",
    "                # Attempt to convert to float\n",
    "                processed_row.append(float(item))\n",
    "            except ValueError:\n",
    "                # If conversion fails, keep it as a string\n",
    "                processed_row.append(item)\n",
    "        meas_data.append(processed_row)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a3f870b",
   "metadata": {},
   "source": [
    "## Contruction des entrées et sorties\n",
    "Ici, les entrées sont les mensurations, et les sorties le genre de la personne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "540d7021-fd5f-4658-a24c-8671d728a869",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create inputs\n",
    "n_obs = len(meas_data)\n",
    "n_features = 6 # poids taille ventre hanche epaule chaussure\n",
    "inputs = np.ndarray((n_obs,n_features))\n",
    "inputs[:,0:6]=[row[2:8] for row in meas_data[:]]\n",
    "# outputs: male=0, female=1\n",
    "gender_col = [row[1] for row in meas_data[:]]\n",
    "gender = np.array([0 if g=='Masculin' else 1 for g in gender_col])\n",
    "# put them in a dictionary\n",
    "gender_data={\"data\":inputs,\"target\":gender}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdce1917-84b1-42d9-997e-daec18027aea",
   "metadata": {},
   "source": [
    "## La famille des fonctions de perte\n",
    "Ci-dessous une collection de fonctions qui servent à générer les fonctions de perte (loss functions). On se sert des variables globales pour passer des paramètres aux fonctions sans qu'ils figurent dans les arguments, ce qui permet de créer une fonction `f(x)` en passant en plus de `x` la structure du réseau ... La variable `__is_logistic__` est aussi globale et sert à ajouter un traitement logistique (par la fonction sigmoïde) en sortie de réseau de neurones. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe401c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# mean squared error\n",
    "def cost_function_mse(y_predicted: np.ndarray,y_observed: np.ndarray):\n",
    "    error = 0.5 * np.mean((y_predicted - y_observed)**2)\n",
    "    return error\n",
    "\n",
    "\n",
    "# entropy\n",
    "# TODO : make it more robust by testing when y_predicted is equal or less than 0, or equal or larger than 1 and\n",
    "#        by not performing the multiplication for the null terms\n",
    "def cost_function_entropy(y_predicted: np.ndarray,y_observed: np.ndarray):\n",
    "\n",
    "    n = len(y_observed)\n",
    "    \n",
    "    term_A = np.multiply(np.log(y_predicted),y_observed)\n",
    "    term_B = np.multiply(1-y_observed,np.log(1-y_predicted))\n",
    "    \n",
    "    error = - (1/n)*(np.sum(term_A)+np.sum(term_B))\n",
    "\n",
    "    return(error)\n",
    "\n",
    "\n",
    "# TODO: I think this function would only work for 1 output because of the reshape(-1) at the end that is not applied to the data.\n",
    "# --> make it multi-dimensional\n",
    "def error_with_parameters(vector_weights: np.ndarray,\n",
    "                          network_structure: List[int],\n",
    "                          activation_function: Callable,\n",
    "                          data: dict,\n",
    "                          cost_function: Callable,\n",
    "                          regularization: float = 0) -> float:\n",
    "    \n",
    "    weights = vector_to_weights(vector_weights,used_network_structure)\n",
    "    predicted_output = forward_propagation(data[\"data\"],weights,activation_function,logistic=__is_logistic__)\n",
    "    predicted_output = predicted_output.reshape(-1,)\n",
    "    \n",
    "    error = cost_function(predicted_output,data[\"target\"]) + regularization * np.sum(np.abs(vector_weights))\n",
    "    \n",
    "    return error\n",
    "\n",
    "\n",
    "def neural_network_cost(vector_weights):\n",
    "    cost = error_with_parameters(vector_weights,\n",
    "                                 network_structure=used_network_structure,\n",
    "                                 activation_function=used_activation,\n",
    "                                 data=used_data,\n",
    "                                 cost_function=used_cost_function)\n",
    "\n",
    "    return cost\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "698e9c6c",
   "metadata": {},
   "source": [
    "## Construction du réseau de neurones\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2209e0f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Create the network\n",
    "# and calculate the cost function of the first, randomly initialized, network.\n",
    "\n",
    "used_network_structure = [6,5,1]\n",
    "used_activation = leaky_relu \n",
    "# other examples of activation functions descriptions:\n",
    "#  [[sigmoid,sigmoid,sigmoid,leaky_relu,leaky_relu],[leaky_relu]]\n",
    "#  [sigmoid,leaky_relu] \n",
    "#  sigmoid or leaky_relu or sigmoid\n",
    "used_data = gender_data # simulated_data\n",
    "used_cost_function = cost_function_entropy # cost_function_mse\n",
    "__is_logistic__ = True\n",
    "weights = create_weights(used_network_structure)\n",
    "weights_as_vector,_ = weights_to_vector(weights)\n",
    "dim = len(weights_as_vector) \n",
    "###  a forward propagation\n",
    "# predicted_output = forward_propagation(inputs=simulated_data[\"data\"],weights=weights,activation_functions=relu,logistic=True)\n",
    "# print(predicted_output)\n",
    "print(\"Number of weights to learn : \",dim)\n",
    "print(\"Initial cost of the NN : \",neural_network_cost(weights_as_vector))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb9a2e4f",
   "metadata": {},
   "source": [
    "## Apprentissage du réseau\n",
    "\n",
    "par minimisation de la fonction perte ou `cost_function_...`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c59a2aed-95e5-46f0-bd1e-37690dc3dcfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "LB = [-8] * dim\n",
    "UB = [8] * dim\n",
    "printlevel = 1\n",
    "# res = gradient_descent(func = neural_network_cost,\n",
    "#                  start_x = weights_as_vector,\n",
    "#                  LB = LB, UB = UB,budget = 100,printlevel=printlevel,\n",
    "#                  min_step_size = 1e-13, min_grad_size = 1e-13, do_linesearch=True,step_factor=0.01, direction_type=\"momentum\"\n",
    "#             )\n",
    "#\n",
    "\n",
    "res = restarted_gradient_descent(func=neural_network_cost, start_x=weights_as_vector,LB=LB,UB=UB,budget=1000,nb_restarts=1,\n",
    "                                 printlevel=printlevel)\n",
    "print_rec(res=res, fun=neural_network_cost, dim=len(res[\"x_best\"]),\n",
    "           LB=LB, UB=UB , printlevel=printlevel, logscale = True)\n",
    "\n",
    "weights_best = vector_to_weights(res[\"x_best\"],used_network_structure)\n",
    "\n",
    "# a small study about the gradients ...\n",
    "# initial gradient\n",
    "init_grad = gradient_finite_diff(func=neural_network_cost , x=weights_as_vector , f_x=neural_network_cost(weights_as_vector))\n",
    "final_grad = gradient_finite_diff(func=neural_network_cost , x=res[\"x_best\"] , f_x=neural_network_cost(res[\"x_best\"]))\n",
    "#\n",
    "\n",
    "\n",
    "print(\"Best NN weights:\",weights_best)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b0f185",
   "metadata": {},
   "source": [
    "# **THE END**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
