{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50e2c7e1",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align: center;\"><strong>TP - Optimisation</strong></h1>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1960d84f",
   "metadata": {},
   "source": [
    "*Auteurs: Rodolphe le Riche, Loris Cros, Armance Darrobers*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd4f97b5",
   "metadata": {},
   "source": [
    "Ce TP a pour objectif d'illustrer par des exemples pratiques le cours d'Optimisation, vu dans le cadre de l'EEIA 2025. Il se déroule en 3 parties successives.\n",
    "Dans la **première partie**, l'objectif sera de coder une version simple de l'algorithme de **descente du gradient**, et de l'appliquer à une fonction convexe, ici f(x)=x², afin de déterminer le minimum de la fonction.\n",
    "\n",
    "Dans la **seconde partie**, un code plus élaboré pour la descente de gradient sera fourni, et les **résultats de minimisation** obtenus pour diverses fonctions pourront être analysés. On analysera notamment l'influence d'un paramètre lambda d'une fonction à optimiser sur les résultats obtenus.\n",
    "\n",
    "Dans un **troisième temps**, on se focalisera sur la **création d'un réseau de neurone**, à partir d'un code fourni. Ici, le code fourni implémentera un réseau de neurone afin d'effectuer une régression sur un jeu de donnée, c'est-à-dire retrouver grâce au réseau de neurone la fonction qui relie les entrées et les sorties de l'échantillon.\n",
    "\n",
    "Enfin, dans une dernière partie, il est attendu que les élèves s'inspirent de cet exemple pour recréer leur propre réseau de neurone ayant un objectif similaire, et en analysent les résultats.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66fdb583",
   "metadata": {},
   "source": [
    "Avant toute chose, on importe les bibliothèques python nécessaires à notre exercice. \n",
    "- *matplotlib.pyplot* est utilisée pour tracer les différents graphes du TP.\n",
    "- *optimcourse* est une bibliothèque \"faite maison\", dans laquelle plusieurs fonctions ont été codées pour qu'elles puissent être ici directement utilisées : une version poussée de l'algorithme de descente du gradient, des exemples de fonction d'activation et des exemples de fonction test...\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87692d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Callable, List\n",
    "\n",
    "from optimcourse.gradient_descent import gradient_descent, gradient_finite_diff\n",
    "from optimcourse.optim_utilities import print_rec\n",
    "from optimcourse.forward_propagation import (\n",
    "    forward_propagation,\n",
    "    create_weights,\n",
    "    vector_to_weights,\n",
    "    weights_to_vector)\n",
    "from optimcourse.activation_functions import (\n",
    "    relu,\n",
    "    sigmoid,\n",
    "    linear,\n",
    "    leaky_relu\n",
    ")\n",
    "from optimcourse.test_functions import (\n",
    "    linear_function,\n",
    "    ackley,\n",
    "    sphere,\n",
    "    quadratic,\n",
    "    rosen,\n",
    "    L1norm,\n",
    "    sphereL1\n",
    ")\n",
    "from optimcourse.restarted_gradient_descent import restarted_gradient_descent\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8781bd19",
   "metadata": {},
   "source": [
    "# Partie 1 : Algorithme de descente de gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "075b7611",
   "metadata": {},
   "source": [
    "Dans cette première partie, l'objectif est d'apprendre à coder une version simple de l'algorithme de descente du gradient en dimension 1. Comme vu dans le cours, l'algorithme de descente de gradient est une méthode d'optimisation qui permet de minimiser une fonction en suivant la direction opposée à celle du gradient. À chaque itération, les paramètres sont ajustés en fonction du gradient de la fonction par rapport à ces paramètres, avec un certain pas d’apprentissage (ou taux d’apprentissage). Ce processus est répété jusqu’à ce que la fonction atteigne un minimum ou qu’un critère d’arrêt soit satisfait. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be291aff",
   "metadata": {},
   "source": [
    "\n",
    "Pour implémenter la fonction de descente de gradient, il est d'abord nécessaire de coder une première fonction auxiliaire qui calcule le gradient du fonction f au niveau d'un point x. Pour cela, on prend en entrée de cette fonction \"dérivée\" un petit pas epsilon. La formule du gradient en dimension 1 est :\n",
    "\n",
    "$$ \\frac{df(x)}{dx} \\simeq \\frac{f(x+\\epsilon)-f(x)}{\\epsilon}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb1ca33",
   "metadata": {},
   "source": [
    "#### Question 1:\n",
    "Coder une fonction derivee(func, x, epsilon), qui prend en entrée une fonction $func$, un point $x$ et un pas $epsilon$, et qui renvoie la dérivée de la fonction $f$ en $x.$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ded1581",
   "metadata": {},
   "outputs": [],
   "source": [
    "def derivee(func, x, epsilon):\n",
    "        \n",
    "    f_x=func(x)\n",
    "    \n",
    "\n",
    "    f_xh=func(x+epsilon)\n",
    "\n",
    "    grad=(f_xh-f_x)/epsilon\n",
    "    return grad\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1669c442",
   "metadata": {},
   "source": [
    "#### Question 2:\n",
    "A partir de cette fonction derivée, afficher la fonction dérivée de la fonction f$$f(x)=x²$$ dans l'intervalle  $[-5 : 5]$\n",
    "\n",
    "*Indice : vous pouvez appliquer votre fonction dérivée à un vecteur x composé de toutes les valeurs de l'axe des abscisses.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "932da10b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def square(x):\n",
    "    return x**2\n",
    "\n",
    "x=np.linspace(-5,5,1000)\n",
    "\n",
    "grd=[derivee(square,x[i],1e-3) for i in range(1000)]\n",
    "\n",
    "# grd=derivee(square,x,1e-3)\n",
    "plt.title(\"Dérivée par petites variations\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(r\"$\\frac{f(x+\\epsilon)-f(x)}{\\epsilon}$\")\n",
    "plt.grid(True)\n",
    "\n",
    "plt.plot(x,grd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c94b5293",
   "metadata": {},
   "source": [
    "#### Question 3 \n",
    "Vérifiez que la fonction tracée ci-dessus corresponde bien à la fonction dérivée de la fonction $f$, qu'on peut trouver analytiquement. Tracer cette fonction dérivée dans l'intervalle  $[-5 : 5].$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ac3fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def square_derivee(x):\n",
    "    return 2*x\n",
    "\n",
    "x=np.linspace(-5,5,1000)\n",
    "\n",
    "grd=square_derivee(x)\n",
    "\n",
    "plt.title(\"Dérivée par méthode analytique\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(r\"$\\frac{df}{dx}$\")\n",
    "plt.grid(True)\n",
    "plt.plot(x,grd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b7d610",
   "metadata": {},
   "source": [
    "Bravo ! A cette étape, vous avez codé une fonction *derivée* qui permet de calculer le gradient d'une fonction en un point."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5247293",
   "metadata": {},
   "source": [
    "Maintenant, on va aller plus loin, en utilisant cette fonction *dérivée* pour coder un algorithme de gradient. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b88be4",
   "metadata": {},
   "source": [
    "#### Question 4\n",
    "Écrivez une fonction nommée $descente-grad(func, x0, pas, epsilon, limit, max_iter)$ qui applique l'algorithme de descente de gradient sur une fonction $func$. L’algorithme commence à partir d’un point initial $x0$ et progresse par pas de taille $pas$. L'exécution s'arrête si la différence entre deux points consécutifs devient inférieure à la valeur seuil $limit$, ou si le nombre maximal d’itérations $max_iter$ est atteint. Le paramètre *epsilon* égal à $10^{-3}$ sera utilisé pour approximer numériquement le gradient de la fonction, grace à la fonction $derivee(func, x, epsilon) $ implémentée précédemment.\n",
    "La fonction devra renvoyer 2 variables : le résultat de la minimisation, ie la valeur de $x $pour laquelle la fonction $f $est minimale, ainsi que la liste de tous les x intermédiaires par lesquels l'algorithme est passé."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b3a4f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def descente_grad(func, x0,pas,epsilon,limit,max_iter):\n",
    "    condition=False\n",
    "    x=x0\n",
    "    conteur=0\n",
    "    liste_x=np.zeros(max_iter)\n",
    "\n",
    "    while not condition :\n",
    "        liste_x[conteur]=x\n",
    "        x_prec=x\n",
    "        der=derivee(func,x,epsilon)\n",
    "        x=x-pas*der\n",
    "\n",
    "        if np.abs(x-x_prec)<limit:\n",
    "            print(\"limite prediction atteinte\")\n",
    "            condition=True\n",
    "        conteur +=1\n",
    "\n",
    "        if conteur > max_iter:\n",
    "            print(\"Limites itérations atteinte\")\n",
    "            condition=True\n",
    "    print(conteur)\n",
    "    return x,liste_x[:conteur]\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce0934cf",
   "metadata": {},
   "source": [
    "#### Question 5:\n",
    "Utilisez votre fonction descente_grad sur la fonction $f(x)=x²$ dans l'intervalle dans $[-5,5]$, et affichez sur un même graphe la fonction f et les valeurs de f(x) intermédiaires obtenues au cours de l'algorithme. On prendra pour valeur initiale $x_0$ un point aléatoire dans $[-5,5]$. Vérifiez que la valeur minimale trouvée par l'algorithme corresponde bien au minimum de la courbe que vous identifierez visuellement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2875eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "x0=np.random.uniform(-3,3)\n",
    "print(x0)\n",
    "x,liste_x=descente_grad(square,x0,0.1,0.0001,1e-8,1000)\n",
    "liste_plot=np.linspace(-3,3,10000)\n",
    "plt.plot(liste_plot,square(liste_plot),color=\"red\",label=\"f(x)\")\n",
    "plt.vlines(x0,0,9,linestyle=\"--\",color=\"k\",label=r\"x_0\")\n",
    "plt.scatter(liste_x,square(liste_x),label=\"f(x) intermédiaires\")\n",
    "plt.ylabel(\"f(x)\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.legend()\n",
    "\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47334154",
   "metadata": {},
   "source": [
    "A cette étape, on a codé pour faire une descente de gradient, qui permet de trouver le minimum d'une fonction $f.$ La descente de gradient est donc une méthode d'optimisation, qui permet de trouver le $minimum$ d'une fonction. On l'utilisera  plus tard dans le cadre de création d'un réseau de neurone, afin de trouver le minimum de la fonction $coût$, qui dépend des $paramètres$ du modèle et des $entrées$ observées. La sortie de l'algorithme d'optimisation nous donnera les valeurs des paramètres du modèle qui permettent de minimiser la fonction coût. Ce sera donc les meilleurs paramètres à utiliser pour notre modèle afin de correspondre au mieux aux données d'entrées."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d42d9ce",
   "metadata": {},
   "source": [
    "# Partie 2: Descente de gradient en dimension 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a268748c",
   "metadata": {},
   "source": [
    "Dans cette partie, pas d'inquiétude, il n'y a pas à coder ! L'objectif est d'observer les résultats de la fonction avancée de descente de gradient, qui fonctionne en plusieurs dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a52451ea",
   "metadata": {},
   "source": [
    "**Remarque 1 :** En général, il est utile de tracer la fonction à optimiser sur un graphe afin d'observer visuellement son minimum.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91da331e",
   "metadata": {},
   "source": [
    "**Remarque 2:** Lorsque l’on cherche à optimiser une fonction en *dimension 1,* cela signifie que la fonction **f** ne dépend que d’une seule variable **x.** On peut alors la représenter graphiquement en traçant f(x) en fonction de x sur un plan 2D.\n",
    "En *dimension 2,* la fonction dépend de deux variables x et y, donc on représente **f(x, y)** à l’aide d’un graphe en 3D.\n",
    "Au-delà de deux dimensions, la visualisation directe devient difficile, voire impossible, et l’analyse se fait alors de manière abstraite ou numérique."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d77abf1a",
   "metadata": {},
   "source": [
    "Le code suivant permet de dessiner les graphes de fonctions à 2 variables. On obtient donc des graphes 3D, et on parlera plus tard d'optimisation de dimension 2 (car on veut optimiser $f$ par rapport à $x_1$ et $x_2$).\n",
    "\n",
    "Plusieurs fonctions de test ont été déjà importée depuis le fichier python *test_functions.py*. Vous pouvez vous amuser à changer le nom du paramètre fun dans le code, afin de changer la fonction qui sera affichée.\n",
    "Pour fun, vous pouvez choisir au choix d'afficher les fonctions suivantes:\n",
    "- linear_function,\n",
    "- ackley,\n",
    "- sphere,\n",
    "- quadratic,\n",
    "- rosen,\n",
    "- L1norm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87a0c93a",
   "metadata": {},
   "source": [
    "#### Question 1:\n",
    " Jouez à tester différentes fonctions, et à observer visuellement le minimum de chacune d'entre elles. Quels commentaires pouvez vous faire sur certaines de ces fonctions ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af33f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function definition\n",
    "dim = 2\n",
    "LB = [-5,-5]\n",
    "UB = [5,5]\n",
    "fun = quadratic # other test_functions can be plotted, cf. test_functions.py\n",
    "\n",
    "  \n",
    "# start drawing the function (necessarily dim==2)\n",
    "no_grid = 100\n",
    "# \n",
    "\n",
    "x1 = np.linspace(start=LB[0], stop=UB[0],num=no_grid)\n",
    "x2 = np.linspace(start=LB[1], stop=UB[1],num=no_grid)\n",
    "x, y = np.meshgrid(x1, x2)\n",
    "xy = np.array([x,y])\n",
    "z = np.apply_along_axis(fun,0,xy)\n",
    "figure = plt.figure()\n",
    "axis = figure.add_subplot(111, projection='3d')\n",
    "#axis.set_zlim(0,150)\n",
    "axis.plot_surface(x, y, z, cmap='jet', shade= \"false\")\n",
    "plt.xlabel(xlabel=\"x1\")\n",
    "plt.ylabel(ylabel=\"x2\")\n",
    "plt.title(label=fun.__name__)\n",
    "axis.set_zlabel(\"f\")\n",
    "plt.show()\n",
    "plt.contour(x,y,z)\n",
    "plt.show()\n",
    "# figure.savefig('plot.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f3560d",
   "metadata": {},
   "source": [
    "Maintenant que nous avons visuellement vu quel avait l'air d'être le minimum de chacune de ces fonctions, nous allons essayer de retrouver la valeur de ce minimum grâce à l'algorithme de la descente de gradient. Pour pouvoir optimiser les fonctions en dimension 2 (ou plus), la fonction déjà implémentée *gradient_descent.py* sera utilisée. \n",
    "Les plus curieux d'entre vous sont libres d'aller regarder dans la fonction importée le code, afin d'oserver comment il a été fait. Le concept reste identique à l'algorithme qui a été implémenté en partie 1 pour la dimension 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb71055f",
   "metadata": {},
   "source": [
    "#### Question 2:\n",
    " Jouez avec la fonction suivante, afin de trouver le minimum de la fonction fun. Vous pouvez vérifier que le minimum de la fonction donné corresponde bien au minimum  observé visuellement à la **question 1.**\n",
    "Comme précédemment, pour fun, vous pouvez choisir au choix d'afficher les fonctions suivantes:\n",
    "- linear_function,\n",
    "- ackley,\n",
    "- sphere,\n",
    "- quadratic,\n",
    "- rosen,\n",
    "- L1norm\n",
    "\n",
    "**Remarque**: Les fonctions peuvent aussi être définies pour des dimensions supérieures à 2, et vous pouvez donc changer la variable dim dans le code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67778975",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################\n",
    "# function definition\n",
    "\n",
    "fun = L1norm\n",
    "dim = 2\n",
    "LB = [-5] * dim\n",
    "UB = [5] * dim\n",
    "# np.random.seed(123) # useful for repeated runs (quadratic fct or initial random point)\n",
    "\n",
    "#########################\n",
    "# algorithms settings\n",
    "# start_x = np.array([3,2,1,-4.5,4.6,-2,-1,4.9,0,2])\n",
    "# start_x = (1+np.arange(dim))*5/dim\n",
    "# start_x = np.array([2.3,4.5])\n",
    "start_x = np.random.uniform(low=LB,high=UB)\n",
    "\n",
    "budget = 1000*(dim+1)\n",
    "printlevel = 1  # =0,1,2 , careful with 2 which is memory consuming\n",
    "\n",
    "#########################\n",
    "# optimize\n",
    "# res = random_opt(func=fun, LB=LB, UB=UB, budget=budget, printlevel=printlevel)\n",
    "res = gradient_descent(func=fun,start_x=start_x, LB=LB,UB=UB,\n",
    "                       budget=budget,\n",
    "                       step_factor=0.1,direction_type=\"momentum\",\n",
    "                       do_linesearch=True,min_step_size=1e-11,\n",
    "                       min_grad_size=1e-6,inertia=0.9,\n",
    "                       printlevel=printlevel)\n",
    "\n",
    "#########################\n",
    "# reporting\n",
    "print_rec(res=res, fun=fun, dim=dim, LB=LB, UB=UB , printlevel=printlevel, logscale = True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "698e9c6c",
   "metadata": {},
   "source": [
    "### Considérons un exemple afin de mieux comprendre le code.\n",
    "\n",
    "Prenons maintenant comme fonction test à minimiser une fonction $f(x)$ couramment utilisée en machine learning :\n",
    "\n",
    "\n",
    "$$ f(x) = \\sum_{i=1}^n (x_i - c_i)^2 + \\lambda \\sum_{i=1}^n \\lvert x_i\\rvert \\quad,\\quad \\lambda \\ge 0 $$\n",
    "$$ c_i = i \\quad \\text{ and } \\quad -5 = LB_i \\le x_i \\le UB_i = 5 \\quad,\\quad i=1,\\ldots,n $$  \n",
    "\n",
    "- *Premier terme:* il s'agit d'une fonction sphérique centrée en $c$. C’est un modèle simplifié de l’erreur quadratique moyenne d’un réseau de neurones, où $c$ correspond aux poids qui minimisent l’erreur d’apprentissage.\n",
    "\n",
    "- *Deuxième terme :* il s’agit de la norme L1 multipliée par $\\lambda$. Les $x_i$ représentent ici les poids d’un réseau de neurones.\n",
    "Ce terme sert à améliorer l’erreur sur les données de test (généralisation).\n",
    "\n",
    "La fonction $f$ est déjà implémentée dans le fichier test_functions.py sous le nom sphereL1.\n",
    "Le paramètre $\\lambda$ est défini directement dans la fonction (vous pouvez consulter le fichier avec votre éditeur Python préféré).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e9ca05f",
   "metadata": {},
   "source": [
    "#### Question 3:\n",
    " Dans un premier temps, afficher la fonction à l'aide du code ci-dessous. Faites varier le paramètre lambda dans le code afin d'observer l'influence du paramètre lambda sur la fonction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2209e0f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function definition\n",
    "\n",
    "lbda=0.1\n",
    "dim = 2\n",
    "LB = [-5,-5]\n",
    "UB = [5,5]\n",
    "fun = lambda x: sphereL1(x,lbda)\n",
    "\n",
    "  \n",
    "# start drawing the function (necessarily dim==2)\n",
    "no_grid = 100\n",
    "# \n",
    "# execute \" %matplotlib qt5 \" in the spyder console for interactive 3D plots \n",
    "# \" %matplotlib inline \" will get back to normal docking\n",
    "x1 = np.linspace(start=LB[0], stop=UB[0],num=no_grid)\n",
    "x2 = np.linspace(start=LB[1], stop=UB[1],num=no_grid)\n",
    "x, y = np.meshgrid(x1, x2)\n",
    "xy = np.array([x,y])\n",
    "z = np.apply_along_axis(fun,0,xy)\n",
    "figure = plt.figure()\n",
    "axis = figure.add_subplot(111, projection='3d')\n",
    "axis.plot_surface(x, y, z, cmap='jet', shade= \"false\")\n",
    "plt.xlabel(xlabel=\"x1\")\n",
    "plt.ylabel(ylabel=\"x2\")\n",
    "plt.title(label=fun.__name__)\n",
    "axis.set_zlabel(\"f\")\n",
    "plt.show()\n",
    "plt.contour(x,y,z)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "076162ce",
   "metadata": {},
   "source": [
    "#### Question 4:\n",
    "Le code suivant est le même que celui vu précédemment. Il permet de calculer le minimum de la fonction à l'aide de l'algorithme de descente du gradient. Utilisez le code pour trouver le minimum de la fonction, et jouez avec le paramètre $lambda$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de0e390",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################\n",
    "# function definition\n",
    "lbda=100\n",
    "fun = lambda x:sphereL1(x,lbda)\n",
    "dim = 10\n",
    "LB = [-5] * dim\n",
    "UB = [5] * dim\n",
    "# np.random.seed(123) # useful for repeated runs (quadratic fct or initial random point)\n",
    "\n",
    "#########################\n",
    "# algorithms settings\n",
    "# start_x = np.array([3,2,1,-4.5,4.6,-2,-1,4.9,0,2])\n",
    "# start_x = (1+np.arange(dim))*5/dim\n",
    "# start_x = np.array([2.3,4.5])\n",
    "start_x = np.random.uniform(low=LB,high=UB)\n",
    "\n",
    "budget = 1000*(dim+1)\n",
    "printlevel = 1  # =0,1,2 , careful with 2 which is memory consuming\n",
    "\n",
    "#########################\n",
    "# optimize\n",
    "# res = random_opt(func=fun, LB=LB, UB=UB, budget=budget, printlevel=printlevel)\n",
    "res = gradient_descent(func=fun,start_x=start_x, LB=LB,UB=UB,\n",
    "                       budget=budget,\n",
    "                       step_factor=0.1,direction_type=\"momentum\",\n",
    "                       do_linesearch=True,min_step_size=1e-11,\n",
    "                       min_grad_size=1e-6,inertia=0.9,\n",
    "                       printlevel=printlevel)\n",
    "\n",
    "#########################\n",
    "# reporting\n",
    "print_rec(res=res, fun=fun, dim=dim, LB=LB, UB=UB , printlevel=printlevel, logscale = True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5979a431",
   "metadata": {},
   "source": [
    "# Partie 3: Création d'un réseau de neurone\n",
    "\n",
    "Dans cette partie, l'objectif est d'abord de comprendre et de s'approprier un code qui permet de créer un réseau de neurone. On pourra en observer les résultats. Dans un second temps, on pourra s'inspirer de ce code afin de créer un nouveau réseau de neurone.\n",
    "Les plus curieux sont encouragés à jeter un coup d'oeil au code de la fonction `forward_propagation`, afin de comprendre en détail son implémentation.\n",
    "\n",
    "Dans cette partie, l’objectif est d’abord de comprendre et de s’approprier un code Python qui permet de construire un réseau de neurones simple. Vous observerez les résultats produits par ce réseau sur un jeu de données d'entrée qu'on construira.\n",
    "\n",
    "Dans un second temps, vous vous appuierez sur ce code comme base pour expérimenter la création et la modification de votre propre réseau de neurones.\n",
    "\n",
    "Les plus curieux sont encouragés à explorer en détail la fonction forward_propagation, qui est au cœur du fonctionnement du réseau. Cela permettra de mieux comprendre comment les données circulent à travers les différentes couches et comment les prédictions sont générées.\n",
    "\n",
    "\n",
    "\n",
    "#### Rappels sur les réseaux de neurones :\n",
    "Un réseau de neurones artificiels est un modèle de machine learning inspiré du fonctionnement du cerveau humain. Il est constitué de neurones organisés en couches :\n",
    "\n",
    "- Une couche d’entrée (input layer), qui reçoit les données initiales,\n",
    "\n",
    "- Une ou plusieurs couches cachées (hidden layers), qui effectuent des transformations intermédiaires,\n",
    "\n",
    "- Une couche de sortie (output layer), qui fournit le résultat du modèle (par exemple : une classe prédite, une valeur continue, etc.).\n",
    "\n",
    "Chaque neurone effectue un calcul simple : il applique une fonction d’activation à une combinaison linéaire pondérée de ses entrées. Les poids (weights) et biais (biases) associés aux connexions entre les neurones sont ajustés pendant l’entraînement grâce à un algorithme de type descente de gradient, afin de minimiser une fonction de coût.\n",
    "\n",
    "Parmi les éléments clés à maîtriser dans un réseau de neurones :\n",
    "\n",
    "- La propagation avant (forward propagation) : calcule la sortie du réseau à partir des entrées,\n",
    "\n",
    "- La rétropropagation (backpropagation) : ajuste les poids en fonction de l’erreur observée,\n",
    "\n",
    "- Les fonctions d’activation : ReLU, sigmoid, tanh, etc., qui influencent la capacité du réseau à apprendre des fonctions complexes,"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97cf1731",
   "metadata": {},
   "source": [
    "### Structure de donnée \n",
    "\n",
    "Le réseau de neurone suivant a 2 couches. La première couche part de 4 données d'entrée dans 3 neurones internes, et la deuxième couche part des 3 sorties du neurone interne et a 2 sorties. Aux biais des neurones sont aussi associés des poids."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d603e4f0",
   "metadata": {},
   "source": [
    "Ici, la méthode de création du réseau de neurone va être constituée des étapes suivantes :\n",
    "\n",
    "**Etape 1 -** D'abord, on va créer notre dataset de données d'entrées. Dans le cas particulier de ce TP, au lieu de prendre des données observées de la vie courante ( par exemple les données de la taille et du poids de personnes ainsi que leur genre), on va créer notre dataset directement nous même, afin d'avoir un jeu de donnée d'entrée. Le dataset de données \"observées\", qu'ici nous allons exeptionnellement créer nous-même, est composé d'un ensemble de variables d'entrées observée ainsi que la sortie observée.\n",
    "Pour reprendre l'exemple précédent : si le réseau de neurone avait pour objectif de prédire le genre d'une personne en fonction de sa taille et son poids, le dataset serait constitué d'un ensemble de couples (entrée: taille et poids ;  sortie : genre) disposés dans une matrice entrée observé X et une matrice sortie Y.\n",
    "Ici, le dataset est créé en prenant des points aléatoires dans les résultat d'une certaine fonction f connue. L'objectif sera donc que le réseau de neurone implémenté arrive à retrouver la fonction f qui relie les entrées et les sorties observées dans le dataset.\n",
    "\n",
    "**Etape 2 -** Dans un second temps, on crée un reseau de neurones avec 1 couche, 2 entrées et une sortie initialisée avec des poids aléatoires. Le modèle du réseau de neurone sera déterminé par une liste de poids, weights. On trouvera la sortie du réseau de neurone à partir des données d'entrées grâce à la fonction **forward_propagation(inputs,weights,activation).** Cette fonction dépend du dataset d'entrée, des poids du modèle de réseau de neurone, et de la fonction d'activation. Pour la fonction d'activation, on prendra la sigmoïde.\n",
    "\n",
    "**Etape 3 -** Une fonction coût sera implémentée. Elle dépend des poids et biais du réseau de neurone, et du jeu de donnée d'entrée. C'est cette fonction qu'il faut minimiser afin de trouver les meilleurs poids et biais du réseau, pour correspondre au mieux au jeu de donnée d'entrée. Ici, deux  fonctions d'erreurs, celle des moindre carrés (nommé mse pour Mean Squared Error) et de cross-entropy.\n",
    "\n",
    "**Etape 4 -** Phase d'optimisation de la fonction coût grâce à l'algorithme de descente de gradient. Cette étape permet de trouver les poids du réseau de neurone qui induisent la fonction coût la plus basse possible. \n",
    "\n",
    "**Etape 5 -** Enfin, une fois que les poids optimaux sont obtenus, on a enfin notre modèle pour le réseau de neurone ! On pourra donc tracer la fonction trouvée et la comparer avec la fonction initiale à partir de laquelle on avait créer notre dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e19ba75",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = np.array([[1,2,5,4],[1,0.2,0.15,0.024]])\n",
    "weights = [\n",
    "        np.array(\n",
    "            [\n",
    "                [1,0.2,0.5,1,-1],\n",
    "                [2,1,3,5,0],\n",
    "                [0.2,0.1,0.6,0.78,1]\n",
    "            ]\n",
    "        ),\n",
    "    np.array(\n",
    "            [\n",
    "                [1,0.2,0.5,1],\n",
    "                [2,1,3,5]\n",
    "            ]\n",
    "        )\n",
    "    ]\n",
    "activation = sigmoid\n",
    "forward_propagation(inputs,weights,activation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c85c50e",
   "metadata": {},
   "source": [
    "## Etape 1 - Création du dataset initial \n",
    "Le dataset est créé en prenant des points aléatoires dans les résultat d'une certaine fonction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e3e73e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_data_target(fun: Callable,\n",
    "                       n_features: int,\n",
    "                       n_obs: int,\n",
    "                       LB: List[float],\n",
    "                       UB: List[float]) -> dict:\n",
    "    \n",
    "    entry_data = np.random.uniform(low= LB,high=UB,size=(n_obs, n_features))\n",
    "    target = np.apply_along_axis(fun, 1, entry_data)\n",
    "    \n",
    "    return {\"data\": entry_data, \"target\": target}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a46295",
   "metadata": {},
   "outputs": [],
   "source": [
    "used_function = linear_function\n",
    "n_features = 2\n",
    "n_obs = 10\n",
    "LB = [-5] * n_features\n",
    "UB = [5] * n_features\n",
    "simulated_data = simulate_data_target(fun = used_function,\n",
    "                                      n_features = n_features,\n",
    "                                      n_obs=n_obs,LB=LB,UB=UB)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9aa3f58",
   "metadata": {},
   "source": [
    "## Etape 2 - Réalisation d'un réseau de neurones, avec une initialisation aléatoire des poids et propagation de ses données d'entrée\n",
    "\n",
    "Creation d'un reseau de neurones avec 1 couche, 2 entrées et une sortie initialisée avec des poids aléatoires."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf3d3b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "network_structure = [2,1]\n",
    "weights = create_weights(network_structure)\n",
    "weights_as_vector,_ = weights_to_vector(weights)\n",
    "dim_weights = len(weights_as_vector)\n",
    "print(\"weights=\",weights)\n",
    "print(\"dim=\",dim_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0afc043e",
   "metadata": {},
   "source": [
    " Propagation des données d'entrée à travers le réseau de neurone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fad5e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_output = forward_propagation(simulated_data[\"data\"],weights,sigmoid)\n",
    "print(predicted_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "311344e9",
   "metadata": {},
   "source": [
    "#### Fonction vecteur to weight \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "696d97e5",
   "metadata": {},
   "source": [
    "La fonction suivante n'est pas utilisée directement dans les calculs mais est utile pour convertir un tableau de données où sont stockés les poids du neurone en un format accepté  par les fonctions du réseau de neurones. Elle sera souvent utilisée en interne des fonctions coûts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de88a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_to_weights([0.28677805, -0.07982693,  0.37394315],network_structure)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "440f1553",
   "metadata": {},
   "source": [
    "## Etape 3 - Définition de la fonction coût"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79b11c8f",
   "metadata": {},
   "source": [
    "Comme vu en cours, nous implémentons deux fonctions d'erreurs, celle des moindre carrés (nommé mse pour Mean Squared Error) et de cross-entropy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d120d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean squared error\n",
    "def cost_function_mse(y_predicted: np.ndarray,y_observed: np.ndarray):\n",
    "    error = 0.5 * np.mean((y_predicted - y_observed)**2)\n",
    "    return error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e981770",
   "metadata": {},
   "outputs": [],
   "source": [
    "# entropy\n",
    "def cost_function_entropy(y_predicted: np.ndarray,y_observed: np.ndarray):\n",
    "\n",
    "    n = len(y_observed)\n",
    "    \n",
    "    term_A = np.multiply(np.log(y_predicted),y_observed)\n",
    "    term_B = np.multiply(1-y_observed,np.log(1-y_predicted))\n",
    "    \n",
    "    error = - (1/n)*(np.sum(term_A)+np.sum(term_B))\n",
    "\n",
    "    return(error)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e2408ce",
   "metadata": {},
   "source": [
    "Implémentation de la fonction erreur générale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d3687a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def error_with_parameters(vector_weights: np.ndarray,\n",
    "                          network_structure: List[int],\n",
    "                          activation_function: Callable,\n",
    "                          data: dict,\n",
    "                          cost_function: Callable,\n",
    "                          regularization: float = 0) -> float:\n",
    "    \n",
    "    weights = vector_to_weights(vector_weights,\n",
    "                                network_structure)\n",
    "    predicted_output = forward_propagation(data[\"data\"],weights,\n",
    "                                           activation_function)\n",
    "    predicted_output = predicted_output.reshape(-1,)\n",
    "    \n",
    "    error = cost_function(predicted_output,data[\"target\"]) + \\\n",
    "    regularization * np.sum(np.abs(vector_weights))\n",
    "    \n",
    "    return error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f975e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Paramètres du réseau de neurone\n",
    "\n",
    "used_network_structure = [2,1] # 2 inputs features, 1 layer with 1 node\n",
    "used_activation = lambda x:x\n",
    "used_data = simulated_data\n",
    "used_cost_function = cost_function_mse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "541b32f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def neural_network_cost(vector_weights):\n",
    "    \n",
    "    cost = error_with_parameters(vector_weights,\n",
    "                                 network_structure = used_network_structure,\n",
    "                                 activation_function = used_activation,\n",
    "                                 data = used_data,\n",
    "                                 cost_function = used_cost_function)\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d4159a",
   "metadata": {},
   "source": [
    "La fonction précédente est la fonction de coût propre à notre réseau de neurones, qui dépend des poids. La phase d'apprentissage de notre réseau de neurone va être celle d'optimisation pour obtenir les meilleurs poids."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5bb4bee",
   "metadata": {},
   "source": [
    "## Etape 4: Optimisation de la fonction coût par l'algorithme de descente de gradient => pour trouver les poids du modèle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6761a75e",
   "metadata": {},
   "source": [
    "### Initialisation des poids :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e3ada7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = create_weights(network_structure)\n",
    "weights_as_vector,_ = weights_to_vector(weights)\n",
    "dim_weights = len(weights_as_vector)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "126a1bb4",
   "metadata": {},
   "source": [
    "#### Calcul du coût avec ce premier guess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35fbce8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "neural_network_cost(weights_as_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5244c01a",
   "metadata": {},
   "source": [
    "#### Détermination du minimum de la fonction coût grâce à la descente de gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b3c7b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "LB = [-5] * dim_weights\n",
    "UB = [5] * dim_weights\n",
    "printlevel = 1\n",
    "res = gradient_descent(func = neural_network_cost,\n",
    "                 start_x = weights_as_vector,\n",
    "                 LB = LB, UB = UB,budget = 1000,printlevel=printlevel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "410566ee",
   "metadata": {},
   "source": [
    "#### Representation graphique de cette convergence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f85caaf5",
   "metadata": {},
   "source": [
    "On représente ici les valeurs de la plus petite valeur de f(x) trouvée, en fonction du numéro d'itération. On peut ainsi voir graphiquement au bout de combien d'itérations est-ce que notre algorithme a trouvé le minimum de la fonction f."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "584ec86a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_rec(res=res, fun=neural_network_cost, dim=len(res[\"x_best\"]), \n",
    "          LB=LB, UB=UB , printlevel=printlevel, logscale = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d00a47f",
   "metadata": {},
   "source": [
    "#### Création de la fonction réseau de neurone avec poids optimaux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ce5615",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_best=vector_to_weights(res[\"x_best\"],network_structure)\n",
    "\n",
    "def RN(x):\n",
    "\n",
    "    return forward_propagation(x,weights_best,used_activation).flatten()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c52ed7da",
   "metadata": {},
   "outputs": [],
   "source": [
    "cost_function_mse(np.apply_along_axis(RN,1,simulated_data[\"data\"]),simulated_data[\"target\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e3c4c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function definition\n",
    "dim = 2\n",
    "LB = [-5,-5]\n",
    "UB = [5,5]\n",
    "fun = RN\n",
    "\n",
    "  \n",
    "# start drawing the function (necessarily dim==2)\n",
    "no_grid = 100\n",
    "# \n",
    "# execute \" %matplotlib qt5 \" in the spyder console for interactive 3D plots \n",
    "# \" %matplotlib inline \" will get back to normal docking\n",
    "x1 = np.linspace(start=LB[0], stop=UB[0],num=no_grid)\n",
    "x2 = np.linspace(start=LB[1], stop=UB[1],num=no_grid)\n",
    "x, y = np.meshgrid(x1, x2)\n",
    "xy = np.array([x,y])\n",
    "z = np.apply_along_axis(fun,0,xy)\n",
    "figure = plt.figure()\n",
    "axis = figure.add_subplot(111, projection='3d')\n",
    "axis.plot_surface(x, y, z, cmap='jet', shade= \"false\")\n",
    "plt.xlabel(xlabel=\"x1\")\n",
    "plt.ylabel(ylabel=\"x2\")\n",
    "plt.title(label=fun.__name__)\n",
    "axis.set_zlabel(\"f\")\n",
    "plt.show()\n",
    "plt.contour(x,y,z)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c1d750",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function definition\n",
    "dim = 2\n",
    "LB = [-5,-5]\n",
    "UB = [5,5]\n",
    "fun = linear_function\n",
    "\n",
    "  \n",
    "# start drawing the function (necessarily dim==2)\n",
    "no_grid = 100\n",
    "# \n",
    "# execute \" %matplotlib qt5 \" in the spyder console for interactive 3D plots \n",
    "# \" %matplotlib inline \" will get back to normal docking\n",
    "x1 = np.linspace(start=LB[0], stop=UB[0],num=no_grid)\n",
    "x2 = np.linspace(start=LB[1], stop=UB[1],num=no_grid)\n",
    "x, y = np.meshgrid(x1, x2)\n",
    "xy = np.array([x,y])\n",
    "z = np.apply_along_axis(fun,0,xy)\n",
    "figure = plt.figure()\n",
    "axis = figure.add_subplot(111, projection='3d')\n",
    "axis.plot_surface(x, y, z, cmap='jet', shade= \"false\")\n",
    "plt.xlabel(xlabel=\"x1\")\n",
    "plt.ylabel(ylabel=\"x2\")\n",
    "plt.title(label=fun.__name__)\n",
    "axis.set_zlabel(\"f\")\n",
    "plt.show()\n",
    "plt.contour(x,y,z)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb0dddf4",
   "metadata": {},
   "source": [
    "# Partie 4: Réalisez votre propre réseau de neurone !  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d828e3",
   "metadata": {},
   "source": [
    "#### Question 1\n",
    "Générez un jeu de données associé à la fonction quadratique en dimension 2 avec 100 points de données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfcccbdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "used_function = quadratic\n",
    "n_features = 2\n",
    "n_obs = 100\n",
    "LB = [-5] * n_features\n",
    "UB = [5] * n_features\n",
    "simulated_data = simulate_data_target(fun = used_function,\n",
    "                                      n_features = n_features,\n",
    "                                      n_obs=n_obs,LB=LB,UB=UB)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "936a53ac",
   "metadata": {},
   "source": [
    "#### Question 2 \n",
    "Créez un réseau de neurones (définition de la fonction coût, initialisation des poids, apprentissage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e24822",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Structure du réseau\n",
    "\n",
    "used_network_structure = [2,5,1]\n",
    "used_activation = relu\n",
    "used_data = simulated_data\n",
    "used_cost_function = cost_function_mse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6faff639",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Définition de la fonction de coût\n",
    "\n",
    "\n",
    "def neural_network_cost(vector_weights):\n",
    "    \n",
    "    cost = error_with_parameters(vector_weights,\n",
    "                                 network_structure = used_network_structure,\n",
    "                                 activation_function = used_activation,\n",
    "                                 data = used_data,\n",
    "                                 cost_function = used_cost_function)\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e746a82c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Initialisation des poids\n",
    "\n",
    "random_weights = create_weights(used_network_structure)\n",
    "random_weights_as_vect,_ = weights_to_vector(random_weights)\n",
    "dim_weights = len(random_weights_as_vect)\n",
    "\n",
    "print(\"Vecteur des poids :\",random_weights_as_vect)\n",
    "print(\"Error :\",neural_network_cost(random_weights_as_vect))\n",
    "print(\"Dimension des poids :\",dim_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a7a5492",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Implémentation de la descente de gradients\n",
    "\n",
    "\n",
    "LB = [-5] * dim_weights\n",
    "UB = [5] * dim_weights\n",
    "printlevel = 1\n",
    "res = gradient_descent(func = neural_network_cost,\n",
    "                 start_x = random_weights_as_vect,\n",
    "                 LB = LB, UB = UB,budget = 15000,printlevel=printlevel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c537a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Affichage de la convergence\n",
    "\n",
    "print_rec(res=res, fun=neural_network_cost, dim=len(res[\"x_best\"]), \n",
    "          LB=LB, UB=UB , printlevel=printlevel, logscale = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd3696f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Redéfinition de la fonction réseau de neurone\n",
    "\n",
    "weights_best=vector_to_weights(res[\"x_best\"],\n",
    "                                used_network_structure)\n",
    "def RN(x):\n",
    "\n",
    "    return forward_propagation(x,weights_best,used_activation).flatten()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e87b41b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cost_function_mse(np.apply_along_axis(RN,1,simulated_data[\"data\"]).flatten(),simulated_data[\"target\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d158f08",
   "metadata": {},
   "source": [
    "#### Question 3:\n",
    " Affichez la fonction de votre réseau de neurone sur un graphique en 3D. Affichez également la fonction avec laquelle vous avez crée le dataset initial. Concluez sur leur ressemblance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d019951",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function definition\n",
    "dim = 2\n",
    "LB = [-5,-5]\n",
    "UB = [5,5]\n",
    "fun = RN\n",
    "\n",
    "  \n",
    "# start drawing the function (necessarily dim==2)\n",
    "no_grid = 100\n",
    "# \n",
    "# execute \" %matplotlib qt5 \" in the spyder console for interactive 3D plots \n",
    "# \" %matplotlib inline \" will get back to normal docking\n",
    "x1 = np.linspace(start=LB[0], stop=UB[0],num=no_grid)\n",
    "x2 = np.linspace(start=LB[1], stop=UB[1],num=no_grid)\n",
    "x, y = np.meshgrid(x1, x2)\n",
    "xy = np.array([x,y])\n",
    "z = np.apply_along_axis(fun,0,xy)\n",
    "figure = plt.figure()\n",
    "axis = figure.add_subplot(111, projection='3d')\n",
    "axis.plot_surface(x, y, z, cmap='jet', shade= \"false\")\n",
    "plt.xlabel(xlabel=\"x1\")\n",
    "plt.ylabel(ylabel=\"x2\")\n",
    "plt.title(label=fun.__name__)\n",
    "axis.set_zlabel(\"f\")\n",
    "plt.show()\n",
    "plt.contour(x,y,z)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def84e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function definition\n",
    "dim = 2\n",
    "LB = [-5,-5]\n",
    "UB = [5,5]\n",
    "fun = quadratic\n",
    "\n",
    "  \n",
    "# start drawing the function (necessarily dim==2)\n",
    "no_grid = 100\n",
    "# \n",
    "# execute \" %matplotlib qt5 \" in the spyder console for interactive 3D plots \n",
    "# \" %matplotlib inline \" will get back to normal docking\n",
    "x1 = np.linspace(start=LB[0], stop=UB[0],num=no_grid)\n",
    "x2 = np.linspace(start=LB[1], stop=UB[1],num=no_grid)\n",
    "x, y = np.meshgrid(x1, x2)\n",
    "xy = np.array([x,y])\n",
    "z = np.apply_along_axis(fun,0,xy)\n",
    "figure = plt.figure()\n",
    "axis = figure.add_subplot(111, projection='3d')\n",
    "axis.plot_surface(x, y, z, cmap='jet', shade= \"false\")\n",
    "plt.xlabel(xlabel=\"x1\")\n",
    "plt.ylabel(ylabel=\"x2\")\n",
    "plt.title(label=fun.__name__)\n",
    "axis.set_zlabel(\"f\")\n",
    "plt.show()\n",
    "plt.contour(x,y,z)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11600427",
   "metadata": {},
   "source": [
    "# **FIN DU NOTEBOOK**\n",
    "Bravo à tous, vous avez maintenant réussi à implémenter votre propre algorithme de descente du gradient, et savez créer un réseau de neurones !"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "621754f4",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
